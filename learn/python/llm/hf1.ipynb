{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name a few capital cities of Africa?',\n",
       " 'https://api-inference.huggingface.co/models/gpt2',\n",
       " [{'generated_text': 'Name a few capital cities of Africa?\\n\\nThe answer is yes.\\n\\nThe capital cities'}]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'inputs': 'The answer to the universe is [MASK].'},\n",
       " 'https://api-inference.huggingface.co/models/bert-base-uncased',\n",
       " [{'score': 0.16964051127433777,\n",
       "   'token': 2053,\n",
       "   'token_str': 'no',\n",
       "   'sequence': 'the answer to the universe is no.'},\n",
       "  {'score': 0.07344779372215271,\n",
       "   'token': 2498,\n",
       "   'token_str': 'nothing',\n",
       "   'sequence': 'the answer to the universe is nothing.'},\n",
       "  {'score': 0.05803246051073074,\n",
       "   'token': 2748,\n",
       "   'token_str': 'yes',\n",
       "   'sequence': 'the answer to the universe is yes.'},\n",
       "  {'score': 0.043957971036434174,\n",
       "   'token': 4242,\n",
       "   'token_str': 'unknown',\n",
       "   'sequence': 'the answer to the universe is unknown.'},\n",
       "  {'score': 0.04015727713704109,\n",
       "   'token': 3722,\n",
       "   'token_str': 'simple',\n",
       "   'sequence': 'the answer to the universe is simple.'}]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'inputs': 'I really enjoyed the movie, it was great!'},\n",
       " 'https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english',\n",
       " [[{'label': 'POSITIVE', 'score': 0.9998735189437866},\n",
       "   {'label': 'NEGATIVE', 'score': 0.0001265111204702407}]]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from os import environ\n",
    "\n",
    "def query(payload, api_url=\"https://api-inference.huggingface.co/models/gpt2\"):\n",
    "    API_TOKEN=environ['HF_ACCESS_TOKEN']\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "    data = json.dumps(payload)\n",
    "    response = requests.request(\"POST\", api_url, headers=headers, data=data)\n",
    "    ans = json.loads(response.content.decode(\"utf-8\"))\n",
    "    display([payload, api_url, ans])\n",
    "    return ans\n",
    "\n",
    "# payload = \"detailed explanation of the benefits and drawbacks of using artificial intelligence in healthcare?\"\n",
    "payload = \"Name a few capital cities of Africa?\"\n",
    "a0 = query(payload)\n",
    "\n",
    "api = \"https://api-inference.huggingface.co/models/bert-base-uncased\"\n",
    "payload = {\"inputs\": \"The answer to the universe is [MASK].\"}\n",
    "query(payload,api_url=api)\n",
    "\n",
    "api = \"https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "payload = {\"inputs\": \"I really enjoyed the movie, it was great!\"}\n",
    "query(payload,api_url=api);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_download_counter'},\n",
       " {'This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub. It takes the name of the category (such as text-classification, depth-estimation, etc), and returns the name of the checkpoint'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'facebook/bart-large-mnli'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_huggingface_tool\n",
    "tool = load_huggingface_tool(\"lysandre/hf-model-downloads\")\n",
    "display([{tool.name} , {tool.description}])\n",
    "tool.run(\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = environ['HF_ACCESS_TOKEN']\n",
    "\n",
    "repo_id = \"google/flan-t5-xl\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "repo_id = \"google/flan-t5-small\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "# tool = load_huggingface_tool(\"lysandre/hf-model-downloads\")\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\": 0, \"max_length\": 64}, client=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Football World Cup was held in 1990. The football world cup was held in 1990. So, the answer is 1990.\n",
      "The Football World Cup was held in 1994. The World Cup was held in 1994. The answer: no.\n",
      "The Football World Cup was held on 13 June 1998. The football world cup was held on 13 June 1998. The answer: yes.\n",
      "The Football World Cup was held in 2002. The football world cup was held in 2002. So, the answer is 2002.\n",
      "The Football World Cup was held on 28 August 2006 in the United States. The World Cup was held on 28 August 2006 in the United States. The football world cup was held on 28 August 2006 in the United States. The answer: 28 August 2006.\n",
      "The Football World Cup was held on 13 August 2010. The FIFA World Cup was held on 13 August 2010. The FIFA World Cup was held on 13 August 2010. The answer: 13 August 2010.\n",
      "The Football World Cup was held on 13 August 2014. The FIFA World Cup was held on 13 August 2014. The FIFA World Cup was held on 13 August 2014. The FIFA World Cup was held on 13 August 2014. The FIFA World Cup was held on 13 August 2014. The FIFA World Cup was held on 13 August 2014. The FIFA World\n",
      "The Football World Cup was held on 29 August 2018. The FIFA World Cup was held on 29 August 2018. The FIFA World Cup was held on 29 August 2018. The answer: 29 August.\n",
      "The football world cup was won by the United States. The football world cup was won by the United States. The answer: United States.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "for yr in range(1990, 2023,4):\n",
    "    xquestion = f\"Who won the football World Cup in the year {yr}? \"\n",
    "    print(llm_chain.run(xquestion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm in!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm a bot, and I love talking about jokes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm a bot, and I love talking about jokes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm a bot, and I love talking about jokes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm a bot, and I love talking about bots.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\", padding_size=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\",)\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt') #, padding='left')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "    # display(bot_input_ids)\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id, )\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfmetal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
